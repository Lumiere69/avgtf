{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning XLM-RoBERTa для Бинарной Классификации\n",
    "\n",
    "В этом ноутбуке мы будем настраивать модель `XLM-RoBERTa` для определения, гармонируют ли два текста поста. Мы будем использовать библиотеку `transformers` от Hugging Face и `datasets` для работы с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Импорт библиотек и инициализация токенайзера и модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "# Инициализируем токенайзер и модель\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Подготовка данных\n",
    "\n",
    "Пример структуры данных: каждый пример состоит из двух текстов и метки (1 — гармонируют, 0 — нет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [\n",
    "    {\"text1\": \"Пример текста поста 1\", \"text2\": \"Пример текста поста 2\", \"label\": 1},\n",
    "    {\"text1\": \"Пример текста поста 3\", \"text2\": \"Пример текста поста 4\", \"label\": 0},\n",
    "    # Добавьте больше данных по необходимости\n",
    "]\n",
    "\n",
    "# Преобразуем данные в Dataset\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Токенизация данных\n",
    "\n",
    "Теперь объединим два текста в один вход для модели и токенизируем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text1\"], examples[\"text2\"], padding=\"max_length\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "# Применяем токенизацию\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")  # Переименовываем колонку для совместимости\n",
    "tokenized_dataset.set_format(\"torch\")  # Устанавливаем формат для PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Разделение на обучающую и тестовую выборки\n",
    "\n",
    "Разделим данные на обучающую и тестовую выборки (80% на обучение и 20% на тест)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем на обучающую и тестовую выборки\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: Настройка параметров обучения и запуск fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки для обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Определяем Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Запускаем обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 6: Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оцениваем модель на тестовой выборке\n",
    "results = trainer.evaluate()\n",
    "print(\"Точность на тестовой выборке:\", results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 7: Предсказание для новых текстов\n",
    "\n",
    "Напишем функцию для предсказания гармоничности двух текстов с использованием обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_harmony(text1, text2):\n",
    "    inputs = tokenizer(text1, text2, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return prediction  # 1 — гармонируют, 0 — нет\n",
    "\n",
    "# Пример предсказания\n",
    "text1 = \"Пример текста поста 1\"\n",
    "text2 = \"Пример текста поста 2\"\n",
    "print(\"Гармонируют ли посты?\", \"Да\" if predict_harmony(text1, text2) == 1 else \"Нет\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
